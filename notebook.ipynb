{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scripts.preprocessing import NanHandler, TrendHandler, OkvedHandler, ColumnRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet('data/train.parquet')\n",
    "test_df = pd.read_parquet('data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing config ###\n",
    "\n",
    "# Running mode \n",
    "split_train = True # whether 'train_df' will be splitted on train and val during current run. Should not be splitted if submittion is expected futher.  \n",
    "\n",
    "# Nan handling \n",
    "del_high_miss_cols = True # Whether to delete columns with high percentage of missing values\n",
    "impute_strategy = None  # None - do not impute, 'avg' - simple imputer (implemented), 'kmeans' - kmeans imputer (not implemented), 'iterative' - iteratieve imputer (not implemented)\n",
    "update_nans = True # Whether to translate values from NanHandler.convert_to_na_dict to np.nan i.e. replace -999 in rko_start_months to np.nan \n",
    "\n",
    "# Trend generator\n",
    "apply_trend_gen = True # Whether to apply trend handler to data (if False, options below will be ignored) -> generates columns '{x}_trend' for x in [unique prefixes of columns that end with _1m, _3m, _1y ...] \n",
    "drop_old_trend_cols = True  # Whether to drop columns out of which trend was calculated i.e. sum_deb_e_oper_1m, cnt_deb_e_oper_1m \n",
    "\n",
    "# Okved handling\n",
    "apply_okved_transform = True # Whether to apply okved handler to data (if False, options below will be ignored) -> generates column 'okved_groups'\n",
    "drop_old_okved = True # Whether to drop original 'okved' column\n",
    "apply_okved_ohe = False # Whether to apply one hot encoding to new transformed column 'okved_groups'\n",
    "\n",
    "\n",
    "col_remover = ColumnRemover()\n",
    "nan_handler = NanHandler(del_high_miss_cols, impute_strategy, update_nans)\n",
    "trend_handler = TrendHandler(apply_handler=apply_trend_gen, drop_old_cols=drop_old_trend_cols)\n",
    "okved_handler = OkvedHandler(drop_old_col=drop_old_okved, apply_ohe=apply_okved_ohe, apply_handler=apply_okved_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ColumnRemover] Dropped columns ['id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TrendHandler] Calculating m3 trends...: 100%|██████████| 32/32 [00:00<00:00, 34.34it/s]\n",
      "[TrendHandler] Calculating m1 trends...: 100%|██████████| 1/1 [00:00<00:00, 21.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrendHandler] Dropped old columns: ['sum_a_oper_1m', 'cnt_a_oper_1m', 'sum_b_oper_1m', 'cnt_b_oper_1m', 'sum_c_oper_1m', 'cnt_c_oper_1m', 'sum_deb_d_oper_1m', 'cnt_deb_d_oper_1m', 'sum_deb_e_oper_1m', 'cnt_deb_e_oper_1m', 'cnt_days_deb_e_oper_1m', 'sum_cred_e_oper_1m', 'cnt_cred_e_oper_1m', 'cnt_days_cred_e_oper_1m', 'sum_deb_f_oper_1m', 'cnt_deb_f_oper_1m', 'cnt_days_deb_f_oper_1m', 'sum_cred_f_oper_1m', 'cnt_cred_f_oper_1m', 'cnt_days_cred_f_oper_1m', 'sum_deb_g_oper_1m', 'cnt_deb_g_oper_1m', 'cnt_days_deb_g_oper_1m', 'sum_cred_g_oper_1m', 'cnt_cred_g_oper_1m', 'cnt_days_cred_g_oper_1m', 'sum_deb_h_oper_1m', 'cnt_deb_h_oper_1m', 'cnt_days_deb_h_oper_1m', 'sum_cred_h_oper_1m', 'cnt_cred_h_oper_1m', 'cnt_days_cred_h_oper_1m', 'sum_a_oper_3m', 'cnt_a_oper_3m', 'sum_b_oper_3m', 'cnt_b_oper_3m', 'sum_c_oper_3m', 'cnt_c_oper_3m', 'sum_deb_d_oper_3m', 'cnt_deb_d_oper_3m', 'sum_deb_e_oper_3m', 'cnt_deb_e_oper_3m', 'cnt_days_deb_e_oper_3m', 'sum_cred_e_oper_3m', 'cnt_cred_e_oper_3m', 'cnt_days_cred_e_oper_3m', 'sum_deb_f_oper_3m', 'cnt_deb_f_oper_3m', 'cnt_days_deb_f_oper_3m', 'sum_cred_f_oper_3m', 'cnt_cred_f_oper_3m', 'cnt_days_cred_f_oper_3m', 'sum_deb_g_oper_3m', 'cnt_deb_g_oper_3m', 'cnt_days_deb_g_oper_3m', 'sum_cred_g_oper_3m', 'cnt_cred_g_oper_3m', 'cnt_days_cred_g_oper_3m', 'sum_deb_h_oper_3m', 'cnt_deb_h_oper_3m', 'cnt_days_deb_h_oper_3m', 'sum_cred_h_oper_3m', 'cnt_cred_h_oper_3m', 'cnt_days_cred_h_oper_3m', 'sum_of_paym_2m', 'sum_of_paym_6m', 'sum_of_paym_1y']\n",
      "[OkvedHandler] Applying okved -> okved_groups transform...\n",
      "[OkvedHandler] Dropped old column 'okved'\n",
      "[ColumnRemover] Dropped columns ['id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TrendHandler] Calculating m3 trends...: 100%|██████████| 32/32 [00:00<00:00, 123.88it/s]\n",
      "[TrendHandler] Calculating m1 trends...: 100%|██████████| 1/1 [00:00<00:00, 71.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrendHandler] Dropped old columns: ['sum_a_oper_1m', 'cnt_a_oper_1m', 'sum_b_oper_1m', 'cnt_b_oper_1m', 'sum_c_oper_1m', 'cnt_c_oper_1m', 'sum_deb_d_oper_1m', 'cnt_deb_d_oper_1m', 'sum_deb_e_oper_1m', 'cnt_deb_e_oper_1m', 'cnt_days_deb_e_oper_1m', 'sum_cred_e_oper_1m', 'cnt_cred_e_oper_1m', 'cnt_days_cred_e_oper_1m', 'sum_deb_f_oper_1m', 'cnt_deb_f_oper_1m', 'cnt_days_deb_f_oper_1m', 'sum_cred_f_oper_1m', 'cnt_cred_f_oper_1m', 'cnt_days_cred_f_oper_1m', 'sum_deb_g_oper_1m', 'cnt_deb_g_oper_1m', 'cnt_days_deb_g_oper_1m', 'sum_cred_g_oper_1m', 'cnt_cred_g_oper_1m', 'cnt_days_cred_g_oper_1m', 'sum_deb_h_oper_1m', 'cnt_deb_h_oper_1m', 'cnt_days_deb_h_oper_1m', 'sum_cred_h_oper_1m', 'cnt_cred_h_oper_1m', 'cnt_days_cred_h_oper_1m', 'sum_a_oper_3m', 'cnt_a_oper_3m', 'sum_b_oper_3m', 'cnt_b_oper_3m', 'sum_c_oper_3m', 'cnt_c_oper_3m', 'sum_deb_d_oper_3m', 'cnt_deb_d_oper_3m', 'sum_deb_e_oper_3m', 'cnt_deb_e_oper_3m', 'cnt_days_deb_e_oper_3m', 'sum_cred_e_oper_3m', 'cnt_cred_e_oper_3m', 'cnt_days_cred_e_oper_3m', 'sum_deb_f_oper_3m', 'cnt_deb_f_oper_3m', 'cnt_days_deb_f_oper_3m', 'sum_cred_f_oper_3m', 'cnt_cred_f_oper_3m', 'cnt_days_cred_f_oper_3m', 'sum_deb_g_oper_3m', 'cnt_deb_g_oper_3m', 'cnt_days_deb_g_oper_3m', 'sum_cred_g_oper_3m', 'cnt_cred_g_oper_3m', 'cnt_days_cred_g_oper_3m', 'sum_deb_h_oper_3m', 'cnt_deb_h_oper_3m', 'cnt_days_deb_h_oper_3m', 'sum_cred_h_oper_3m', 'cnt_cred_h_oper_3m', 'cnt_days_cred_h_oper_3m', 'sum_of_paym_2m', 'sum_of_paym_6m', 'sum_of_paym_1y']\n",
      "[OkvedHandler] Applying okved -> okved_groups transform...\n",
      "[OkvedHandler] Dropped old column 'okved'\n"
     ]
    }
   ],
   "source": [
    "col_remover.process(train_df)\n",
    "trend_handler.process(train_df)\n",
    "okved_handler.process(train_df)\n",
    "\n",
    "col_remover.process(test_df)\n",
    "trend_handler.process(test_df)\n",
    "okved_handler.process(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_train:\n",
    "    train_indices, val_indices = train_test_split(list(train_df.index), stratify=train_df['total_target'], test_size=0.2, random_state=42)\n",
    "    val_df = train_df.iloc[val_indices]\n",
    "    train_df = train_df.iloc[train_indices]\n",
    "    X_train, y1_train, y2_train, y_train = train_df.drop(['target_1', 'target_2', 'total_target'], axis=1), train_df['target_1'], train_df['target_2'], train_df['total_target']\n",
    "    X_val, y1_val, y2_val, y_val = val_df.drop(['target_1', 'target_2', 'total_target'], axis=1), val_df['target_1'], val_df['target_2'], val_df['total_target']\n",
    "else:\n",
    "    X_train, y1_train, y2_train, y_train = train_df.drop(['target_1', 'target_2', 'total_target'], axis=1), train_df['target_1'], train_df['target_2'], train_df['total_target']\n",
    "\n",
    "X_test = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NanHandler] Deleting high percentage nan columns: ['max_end_plan_non_fin_deals', 'max_start_non_fin_deals', 'min_start_non_fin_deals', 'min_end_plan_non_fin_deals', 'max_start_fin_deals', 'min_end_fact_fin_deals', 'min_start_fin_deals', 'max_end_fact_fin_deals']\n",
      "[NanHandler] Converted value -999 from column rko_start_months to np.nan\n",
      "[NanHandler] Deleting high percentage nan columns: ['max_end_plan_non_fin_deals', 'max_start_non_fin_deals', 'min_start_non_fin_deals', 'min_end_plan_non_fin_deals', 'max_start_fin_deals', 'min_end_fact_fin_deals', 'min_start_fin_deals', 'max_end_fact_fin_deals']\n",
      "[NanHandler] Converted value -999 from column rko_start_months to np.nan\n",
      "[NanHandler] Deleting high percentage nan columns: ['max_end_plan_non_fin_deals', 'max_start_non_fin_deals', 'min_start_non_fin_deals', 'min_end_plan_non_fin_deals', 'max_start_fin_deals', 'min_end_fact_fin_deals', 'min_start_fin_deals', 'max_end_fact_fin_deals']\n",
      "[NanHandler] Converted value -999 from column rko_start_months to np.nan\n"
     ]
    }
   ],
   "source": [
    "nan_handler.process(X_train, is_train=True)\n",
    "nan_handler.process(X_val, is_train=False)\n",
    "nan_handler.process(X_test, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df.columns.difference(['id', 'target_1', 'target_2'])]\n",
    "test_df_ids = test_df.id\n",
    "test_df = test_df[test_df.columns.difference(['id'])]\n",
    "\n",
    "cat_cols = [\n",
    "    'channel_code', 'city', 'city_type',\n",
    "    'index_city_code', 'ogrn_month', 'ogrn_year',\n",
    "    'branch_code', 'segment',  #'okved_groups'\n",
    "]\n",
    "cont_cols = list(train_df.columns.difference(cat_cols + ['total_target']))\n",
    "train_df[cat_cols] = train_df[cat_cols].astype(\"category\")\n",
    "test_df[cat_cols] = test_df[cat_cols].astype(\"category\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
